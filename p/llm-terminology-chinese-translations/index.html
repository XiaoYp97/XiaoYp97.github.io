<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="模型名称及其中英文对应 英文名称 中文对应 全称 中文全称翻译 GPT-3/GPT-4 GPT-3/GPT-4 Generative Pre-trained Transformer 3/4 生成式预训练变换器 3/4 BERT BERT Bidirectional Encoder Representations from Transformers 双向编码器表示来自变换器 RoBERTa RoBERTa Robustly Optimized BERT Pre-Training Approach 稳健优化 BERT 预训练方法 XLNet XLNet XLNet XLNet T5 T5 Text-to-Text Transfer Transformer 文本到文本转移变换器 BART BART Bidirectional and Auto-Regressive Transformers 双向和自回归变换器 ELECTRA ELECTRA Efficiently Learning an Encoder that Classifies Token Replacements Accurately 高效学习编码器分类令牌替换准确 DeBERTa DeBERTa Decoding-enhanced BERT with Disentangled Attention 解码增强 BERT 具有解耦注意力 PaLM PaLM Pathways Language Model 路径语言模型 LLaMA LLaMA Large Language Model Meta AI 大型语言模型 Meta AI BLOOM BLOOM BigScience Large Open-science Open-access Multilangual language model BigScience 大型开源多语言语言模型 Chinchilla Chinchilla Chinchilla Chinchilla OPT OPT Open Pre-trained Transformer 开放预训练变换器 ERNIE ERNIE（文心一言） Enhanced Representation through Knowledge Integration 通过知识集成增强表示 CPM CPM Chinese Pre-trained Model 中文预训练模型 GLM GLM General Language Model 通用语言模型 MacBert MacBert MacBert MacBert Chinese BERT Chinese BERT Chinese BERT 中文 BERT 术语及其中英文对应 英文术语 中文翻译 说明 AIGC 人工智能生成内容 通过AI技术自动生成的各类媒体内容（文本、图像、音频等） Large Language Model 大型语言模型 LLM 的核心概念 Pre training 预训练 模型在大量数据上的初始训练 Fine-tuning 微调 在特定任务上调整模型 Transformer 变换器 LLM 常用的架构 Attention mechanism 注意力机制 变换器中的关键技术 Word embedding 词嵌入 单词到向量表示 Contextualized embedding 上下文嵌入 考虑上下文的嵌入 Masked language modeling 掩码语言建模 BERT 使用的预训练任务 Next sentence prediction 下一句预测 BERT 早期使用的任务 Transfer learning 迁移学习 模型在不同任务间的应用 Natural language processing 自然语言处理 LLM 的应用领域 Deep learning 深度学习 LLM 的基础技术 Neural network 神经网络 深度学习的构建块 Gradient descent 梯度下降 优化算法 Backpropagation 反向传播 训练神经网络的核心方法 Prompt 提示 输入给模型的指令 Prompt engineering 提示工程 优化提示以改善输出 Few-shot learning 少样本学习 少量示例下学习 Zero-shot learning 零样本学习 无示例直接推理 In-context learning 上下文学习 基于上下文的推理 Parameter-efficient fine-tuning 参数高效微调 减少参数调整的微调方法 Model scaling 模型扩展 增加模型规模以提升性能 Compute-optimal scaling 计算最优扩展 优化计算资源的使用 Data-optimal scaling 数据最优扩展 优化数据使用的扩展 Encoder-decoder architecture 编码解码器架构 某些 LLM 的架构 Auto-regressive model 自回归模型 生成文本的模型类型 Bidirectional model 双向模型 考虑前后文的模型 Self-supervision 自监督 无需标签的训练方法 Unsupervised learning 无监督学习 无标签数据的学习 Supervised learning 监督学习 有标签数据的学习 "><title>LLM领域的术语及其中文翻译</title><link rel=canonical href=https://blog.yellster.top/p/llm-terminology-chinese-translations/><link rel=stylesheet href=/scss/style.min.946cca6c6259ef94ac55abfae7c7bf3291ea3ed5eea17ef77500b257217c6710.css><meta property='og:title' content="LLM领域的术语及其中文翻译"><meta property='og:description' content="模型名称及其中英文对应 英文名称 中文对应 全称 中文全称翻译 GPT-3/GPT-4 GPT-3/GPT-4 Generative Pre-trained Transformer 3/4 生成式预训练变换器 3/4 BERT BERT Bidirectional Encoder Representations from Transformers 双向编码器表示来自变换器 RoBERTa RoBERTa Robustly Optimized BERT Pre-Training Approach 稳健优化 BERT 预训练方法 XLNet XLNet XLNet XLNet T5 T5 Text-to-Text Transfer Transformer 文本到文本转移变换器 BART BART Bidirectional and Auto-Regressive Transformers 双向和自回归变换器 ELECTRA ELECTRA Efficiently Learning an Encoder that Classifies Token Replacements Accurately 高效学习编码器分类令牌替换准确 DeBERTa DeBERTa Decoding-enhanced BERT with Disentangled Attention 解码增强 BERT 具有解耦注意力 PaLM PaLM Pathways Language Model 路径语言模型 LLaMA LLaMA Large Language Model Meta AI 大型语言模型 Meta AI BLOOM BLOOM BigScience Large Open-science Open-access Multilangual language model BigScience 大型开源多语言语言模型 Chinchilla Chinchilla Chinchilla Chinchilla OPT OPT Open Pre-trained Transformer 开放预训练变换器 ERNIE ERNIE（文心一言） Enhanced Representation through Knowledge Integration 通过知识集成增强表示 CPM CPM Chinese Pre-trained Model 中文预训练模型 GLM GLM General Language Model 通用语言模型 MacBert MacBert MacBert MacBert Chinese BERT Chinese BERT Chinese BERT 中文 BERT 术语及其中英文对应 英文术语 中文翻译 说明 AIGC 人工智能生成内容 通过AI技术自动生成的各类媒体内容（文本、图像、音频等） Large Language Model 大型语言模型 LLM 的核心概念 Pre training 预训练 模型在大量数据上的初始训练 Fine-tuning 微调 在特定任务上调整模型 Transformer 变换器 LLM 常用的架构 Attention mechanism 注意力机制 变换器中的关键技术 Word embedding 词嵌入 单词到向量表示 Contextualized embedding 上下文嵌入 考虑上下文的嵌入 Masked language modeling 掩码语言建模 BERT 使用的预训练任务 Next sentence prediction 下一句预测 BERT 早期使用的任务 Transfer learning 迁移学习 模型在不同任务间的应用 Natural language processing 自然语言处理 LLM 的应用领域 Deep learning 深度学习 LLM 的基础技术 Neural network 神经网络 深度学习的构建块 Gradient descent 梯度下降 优化算法 Backpropagation 反向传播 训练神经网络的核心方法 Prompt 提示 输入给模型的指令 Prompt engineering 提示工程 优化提示以改善输出 Few-shot learning 少样本学习 少量示例下学习 Zero-shot learning 零样本学习 无示例直接推理 In-context learning 上下文学习 基于上下文的推理 Parameter-efficient fine-tuning 参数高效微调 减少参数调整的微调方法 Model scaling 模型扩展 增加模型规模以提升性能 Compute-optimal scaling 计算最优扩展 优化计算资源的使用 Data-optimal scaling 数据最优扩展 优化数据使用的扩展 Encoder-decoder architecture 编码解码器架构 某些 LLM 的架构 Auto-regressive model 自回归模型 生成文本的模型类型 Bidirectional model 双向模型 考虑前后文的模型 Self-supervision 自监督 无需标签的训练方法 Unsupervised learning 无监督学习 无标签数据的学习 Supervised learning 监督学习 有标签数据的学习 "><meta property='og:url' content='https://blog.yellster.top/p/llm-terminology-chinese-translations/'><meta property='og:site_name' content='Yellster - Blog'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='LLM'><meta property='article:tag' content='AIGC'><meta property='article:tag' content='AI'><meta property='article:tag' content='中文翻译'><meta property='article:tag' content='技术术语'><meta property='article:published_time' content='2025-03-24T22:30:59+08:00'><meta property='article:modified_time' content='2025-03-24T22:30:59+08:00'><meta name=twitter:title content="LLM领域的术语及其中文翻译"><meta name=twitter:description content="模型名称及其中英文对应 英文名称 中文对应 全称 中文全称翻译 GPT-3/GPT-4 GPT-3/GPT-4 Generative Pre-trained Transformer 3/4 生成式预训练变换器 3/4 BERT BERT Bidirectional Encoder Representations from Transformers 双向编码器表示来自变换器 RoBERTa RoBERTa Robustly Optimized BERT Pre-Training Approach 稳健优化 BERT 预训练方法 XLNet XLNet XLNet XLNet T5 T5 Text-to-Text Transfer Transformer 文本到文本转移变换器 BART BART Bidirectional and Auto-Regressive Transformers 双向和自回归变换器 ELECTRA ELECTRA Efficiently Learning an Encoder that Classifies Token Replacements Accurately 高效学习编码器分类令牌替换准确 DeBERTa DeBERTa Decoding-enhanced BERT with Disentangled Attention 解码增强 BERT 具有解耦注意力 PaLM PaLM Pathways Language Model 路径语言模型 LLaMA LLaMA Large Language Model Meta AI 大型语言模型 Meta AI BLOOM BLOOM BigScience Large Open-science Open-access Multilangual language model BigScience 大型开源多语言语言模型 Chinchilla Chinchilla Chinchilla Chinchilla OPT OPT Open Pre-trained Transformer 开放预训练变换器 ERNIE ERNIE（文心一言） Enhanced Representation through Knowledge Integration 通过知识集成增强表示 CPM CPM Chinese Pre-trained Model 中文预训练模型 GLM GLM General Language Model 通用语言模型 MacBert MacBert MacBert MacBert Chinese BERT Chinese BERT Chinese BERT 中文 BERT 术语及其中英文对应 英文术语 中文翻译 说明 AIGC 人工智能生成内容 通过AI技术自动生成的各类媒体内容（文本、图像、音频等） Large Language Model 大型语言模型 LLM 的核心概念 Pre training 预训练 模型在大量数据上的初始训练 Fine-tuning 微调 在特定任务上调整模型 Transformer 变换器 LLM 常用的架构 Attention mechanism 注意力机制 变换器中的关键技术 Word embedding 词嵌入 单词到向量表示 Contextualized embedding 上下文嵌入 考虑上下文的嵌入 Masked language modeling 掩码语言建模 BERT 使用的预训练任务 Next sentence prediction 下一句预测 BERT 早期使用的任务 Transfer learning 迁移学习 模型在不同任务间的应用 Natural language processing 自然语言处理 LLM 的应用领域 Deep learning 深度学习 LLM 的基础技术 Neural network 神经网络 深度学习的构建块 Gradient descent 梯度下降 优化算法 Backpropagation 反向传播 训练神经网络的核心方法 Prompt 提示 输入给模型的指令 Prompt engineering 提示工程 优化提示以改善输出 Few-shot learning 少样本学习 少量示例下学习 Zero-shot learning 零样本学习 无示例直接推理 In-context learning 上下文学习 基于上下文的推理 Parameter-efficient fine-tuning 参数高效微调 减少参数调整的微调方法 Model scaling 模型扩展 增加模型规模以提升性能 Compute-optimal scaling 计算最优扩展 优化计算资源的使用 Data-optimal scaling 数据最优扩展 优化数据使用的扩展 Encoder-decoder architecture 编码解码器架构 某些 LLM 的架构 Auto-regressive model 自回归模型 生成文本的模型类型 Bidirectional model 双向模型 考虑前后文的模型 Self-supervision 自监督 无需标签的训练方法 Unsupervised learning 无监督学习 无标签数据的学习 Supervised learning 监督学习 有标签数据的学习 "><link rel="shortcut icon" href=https://avatars.githubusercontent.com/u/37898221></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky compact"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src="https://avatars.githubusercontent.com/u/37898221?v=4" width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🇨🇳</span></figure><div class=site-meta><h1 class=site-name><a href=/>Yellster - Blog</a></h1><h2 class=site-description>想都是问题，做才是答案</h2></div></header><ol class=menu-social><li><a href=https://github.com/Yellster target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于</span></a></li><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>首页</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>链接</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#模型名称及其中英文对应>模型名称及其中英文对应</a></li><li><a href=#术语及其中英文对应>术语及其中英文对应</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/llm/>LLM
</a><a href=/categories/aigc/>AIGC
</a><a href=/categories/ai/>AI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/llm-terminology-chinese-translations/>LLM领域的术语及其中文翻译</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2025-03-24</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 2 分钟</time></div></footer></div></header><section class=article-content><h2 id=模型名称及其中英文对应>模型名称及其中英文对应</h2><div class=table-wrapper><table><thead><tr><th><strong>英文名称</strong></th><th><strong>中文对应</strong></th><th><strong>全称</strong></th><th><strong>中文全称翻译</strong></th></tr></thead><tbody><tr><td>GPT-3/GPT-4</td><td>GPT-3/GPT-4</td><td>Generative Pre-trained Transformer 3/4</td><td>生成式预训练变换器 3/4</td></tr><tr><td>BERT</td><td>BERT</td><td>Bidirectional Encoder Representations from Transformers</td><td>双向编码器表示来自变换器</td></tr><tr><td>RoBERTa</td><td>RoBERTa</td><td>Robustly Optimized BERT Pre-Training Approach</td><td>稳健优化 BERT 预训练方法</td></tr><tr><td>XLNet</td><td>XLNet</td><td>XLNet</td><td>XLNet</td></tr><tr><td>T5</td><td>T5</td><td>Text-to-Text Transfer Transformer</td><td>文本到文本转移变换器</td></tr><tr><td>BART</td><td>BART</td><td>Bidirectional and Auto-Regressive Transformers</td><td>双向和自回归变换器</td></tr><tr><td>ELECTRA</td><td>ELECTRA</td><td>Efficiently Learning an Encoder that Classifies Token Replacements Accurately</td><td>高效学习编码器分类令牌替换准确</td></tr><tr><td>DeBERTa</td><td>DeBERTa</td><td>Decoding-enhanced BERT with Disentangled Attention</td><td>解码增强 BERT 具有解耦注意力</td></tr><tr><td>PaLM</td><td>PaLM</td><td>Pathways Language Model</td><td>路径语言模型</td></tr><tr><td>LLaMA</td><td>LLaMA</td><td>Large Language Model Meta AI</td><td>大型语言模型 Meta AI</td></tr><tr><td>BLOOM</td><td>BLOOM</td><td>BigScience Large Open-science Open-access Multilangual language model</td><td>BigScience 大型开源多语言语言模型</td></tr><tr><td>Chinchilla</td><td>Chinchilla</td><td>Chinchilla</td><td>Chinchilla</td></tr><tr><td>OPT</td><td>OPT</td><td>Open Pre-trained Transformer</td><td>开放预训练变换器</td></tr><tr><td>ERNIE</td><td>ERNIE（文心一言）</td><td>Enhanced Representation through Knowledge Integration</td><td>通过知识集成增强表示</td></tr><tr><td>CPM</td><td>CPM</td><td>Chinese Pre-trained Model</td><td>中文预训练模型</td></tr><tr><td>GLM</td><td>GLM</td><td>General Language Model</td><td>通用语言模型</td></tr><tr><td>MacBert</td><td>MacBert</td><td>MacBert</td><td>MacBert</td></tr><tr><td>Chinese BERT</td><td>Chinese BERT</td><td>Chinese BERT</td><td>中文 BERT</td></tr></tbody></table></div><h2 id=术语及其中英文对应>术语及其中英文对应</h2><div class=table-wrapper><table><thead><tr><th>英文术语</th><th>中文翻译</th><th>说明</th></tr></thead><tbody><tr><td>AIGC</td><td>人工智能生成内容</td><td>通过AI技术自动生成的各类媒体内容（文本、图像、音频等）</td></tr><tr><td>Large Language Model</td><td>大型语言模型</td><td>LLM 的核心概念</td></tr><tr><td>Pre training</td><td>预训练</td><td>模型在大量数据上的初始训练</td></tr><tr><td>Fine-tuning</td><td>微调</td><td>在特定任务上调整模型</td></tr><tr><td>Transformer</td><td>变换器</td><td>LLM 常用的架构</td></tr><tr><td>Attention mechanism</td><td>注意力机制</td><td>变换器中的关键技术</td></tr><tr><td>Word embedding</td><td>词嵌入</td><td>单词到向量表示</td></tr><tr><td>Contextualized embedding</td><td>上下文嵌入</td><td>考虑上下文的嵌入</td></tr><tr><td>Masked language modeling</td><td>掩码语言建模</td><td>BERT 使用的预训练任务</td></tr><tr><td>Next sentence prediction</td><td>下一句预测</td><td>BERT 早期使用的任务</td></tr><tr><td>Transfer learning</td><td>迁移学习</td><td>模型在不同任务间的应用</td></tr><tr><td>Natural language processing</td><td>自然语言处理</td><td>LLM 的应用领域</td></tr><tr><td>Deep learning</td><td>深度学习</td><td>LLM 的基础技术</td></tr><tr><td>Neural network</td><td>神经网络</td><td>深度学习的构建块</td></tr><tr><td>Gradient descent</td><td>梯度下降</td><td>优化算法</td></tr><tr><td>Backpropagation</td><td>反向传播</td><td>训练神经网络的核心方法</td></tr><tr><td>Prompt</td><td>提示</td><td>输入给模型的指令</td></tr><tr><td>Prompt engineering</td><td>提示工程</td><td>优化提示以改善输出</td></tr><tr><td>Few-shot learning</td><td>少样本学习</td><td>少量示例下学习</td></tr><tr><td>Zero-shot learning</td><td>零样本学习</td><td>无示例直接推理</td></tr><tr><td>In-context learning</td><td>上下文学习</td><td>基于上下文的推理</td></tr><tr><td>Parameter-efficient fine-tuning</td><td>参数高效微调</td><td>减少参数调整的微调方法</td></tr><tr><td>Model scaling</td><td>模型扩展</td><td>增加模型规模以提升性能</td></tr><tr><td>Compute-optimal scaling</td><td>计算最优扩展</td><td>优化计算资源的使用</td></tr><tr><td>Data-optimal scaling</td><td>数据最优扩展</td><td>优化数据使用的扩展</td></tr><tr><td>Encoder-decoder architecture</td><td>编码解码器架构</td><td>某些 LLM 的架构</td></tr><tr><td>Auto-regressive model</td><td>自回归模型</td><td>生成文本的模型类型</td></tr><tr><td>Bidirectional model</td><td>双向模型</td><td>考虑前后文的模型</td></tr><tr><td>Self-supervision</td><td>自监督</td><td>无需标签的训练方法</td></tr><tr><td>Unsupervised learning</td><td>无监督学习</td><td>无标签数据的学习</td></tr><tr><td>Supervised learning</td><td>监督学习</td><td>有标签数据的学习</td></tr></tbody></table></div></section><footer class=article-footer><section class=article-tags><a href=/tags/llm/>LLM</a>
<a href=/tags/aigc/>AIGC</a>
<a href=/tags/ai/>AI</a>
<a href=/tags/%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91/>中文翻译</a>
<a href=/tags/%E6%8A%80%E6%9C%AF%E6%9C%AF%E8%AF%AD/>技术术语</a></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><footer class=site-footer><section class=copyright>&copy;
2023 -
2025 Yellster - Blog</section><section class=powerby>想都是问题，做才是答案！<br>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>