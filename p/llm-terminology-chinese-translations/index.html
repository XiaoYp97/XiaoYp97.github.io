<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="æ¨¡å‹åç§°åŠå…¶ä¸­è‹±æ–‡å¯¹åº” è‹±æ–‡åç§° ä¸­æ–‡å¯¹åº” å…¨ç§° ä¸­æ–‡å…¨ç§°ç¿»è¯‘ GPT-3/GPT-4 GPT-3/GPT-4 Generative Pre-trained Transformer 3/4 ç”Ÿæˆå¼é¢„è®­ç»ƒå˜æ¢å™¨ 3/4 BERT BERT Bidirectional Encoder Representations from Transformers åŒå‘ç¼–ç å™¨è¡¨ç¤ºæ¥è‡ªå˜æ¢å™¨ RoBERTa RoBERTa Robustly Optimized BERT Pre-Training Approach ç¨³å¥ä¼˜åŒ– BERT é¢„è®­ç»ƒæ–¹æ³• XLNet XLNet XLNet XLNet T5 T5 Text-to-Text Transfer Transformer æ–‡æœ¬åˆ°æ–‡æœ¬è½¬ç§»å˜æ¢å™¨ BART BART Bidirectional and Auto-Regressive Transformers åŒå‘å’Œè‡ªå›å½’å˜æ¢å™¨ ELECTRA ELECTRA Efficiently Learning an Encoder that Classifies Token Replacements Accurately é«˜æ•ˆå­¦ä¹ ç¼–ç å™¨åˆ†ç±»ä»¤ç‰Œæ›¿æ¢å‡†ç¡® DeBERTa DeBERTa Decoding-enhanced BERT with Disentangled Attention è§£ç å¢å¼º BERT å…·æœ‰è§£è€¦æ³¨æ„åŠ› PaLM PaLM Pathways Language Model è·¯å¾„è¯­è¨€æ¨¡å‹ LLaMA LLaMA Large Language Model Meta AI å¤§å‹è¯­è¨€æ¨¡å‹ Meta AI BLOOM BLOOM BigScience Large Open-science Open-access Multilangual language model BigScience å¤§å‹å¼€æºå¤šè¯­è¨€è¯­è¨€æ¨¡å‹ Chinchilla Chinchilla Chinchilla Chinchilla OPT OPT Open Pre-trained Transformer å¼€æ”¾é¢„è®­ç»ƒå˜æ¢å™¨ ERNIE ERNIEï¼ˆæ–‡å¿ƒä¸€è¨€ï¼‰ Enhanced Representation through Knowledge Integration é€šè¿‡çŸ¥è¯†é›†æˆå¢å¼ºè¡¨ç¤º CPM CPM Chinese Pre-trained Model ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ GLM GLM General Language Model é€šç”¨è¯­è¨€æ¨¡å‹ MacBert MacBert MacBert MacBert Chinese BERT Chinese BERT Chinese BERT ä¸­æ–‡ BERT æœ¯è¯­åŠå…¶ä¸­è‹±æ–‡å¯¹åº” è‹±æ–‡æœ¯è¯­ ä¸­æ–‡ç¿»è¯‘ è¯´æ˜ AIGC äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ é€šè¿‡AIæŠ€æœ¯è‡ªåŠ¨ç”Ÿæˆçš„å„ç±»åª’ä½“å†…å®¹ï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰ï¼‰ Large Language Model å¤§å‹è¯­è¨€æ¨¡å‹ LLM çš„æ ¸å¿ƒæ¦‚å¿µ Pre training é¢„è®­ç»ƒ æ¨¡å‹åœ¨å¤§é‡æ•°æ®ä¸Šçš„åˆå§‹è®­ç»ƒ Fine-tuning å¾®è°ƒ åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè°ƒæ•´æ¨¡å‹ Transformer å˜æ¢å™¨ LLM å¸¸ç”¨çš„æ¶æ„ Attention mechanism æ³¨æ„åŠ›æœºåˆ¶ å˜æ¢å™¨ä¸­çš„å…³é”®æŠ€æœ¯ Word embedding è¯åµŒå…¥ å•è¯åˆ°å‘é‡è¡¨ç¤º Contextualized embedding ä¸Šä¸‹æ–‡åµŒå…¥ è€ƒè™‘ä¸Šä¸‹æ–‡çš„åµŒå…¥ Masked language modeling æ©ç è¯­è¨€å»ºæ¨¡ BERT ä½¿ç”¨çš„é¢„è®­ç»ƒä»»åŠ¡ Next sentence prediction ä¸‹ä¸€å¥é¢„æµ‹ BERT æ—©æœŸä½¿ç”¨çš„ä»»åŠ¡ Transfer learning è¿ç§»å­¦ä¹  æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡é—´çš„åº”ç”¨ Natural language processing è‡ªç„¶è¯­è¨€å¤„ç† LLM çš„åº”ç”¨é¢†åŸŸ Deep learning æ·±åº¦å­¦ä¹  LLM çš„åŸºç¡€æŠ€æœ¯ Neural network ç¥ç»ç½‘ç»œ æ·±åº¦å­¦ä¹ çš„æ„å»ºå— Gradient descent æ¢¯åº¦ä¸‹é™ ä¼˜åŒ–ç®—æ³• Backpropagation åå‘ä¼ æ’­ è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ–¹æ³• Prompt æç¤º è¾“å…¥ç»™æ¨¡å‹çš„æŒ‡ä»¤ Prompt engineering æç¤ºå·¥ç¨‹ ä¼˜åŒ–æç¤ºä»¥æ”¹å–„è¾“å‡º Few-shot learning å°‘æ ·æœ¬å­¦ä¹  å°‘é‡ç¤ºä¾‹ä¸‹å­¦ä¹  Zero-shot learning é›¶æ ·æœ¬å­¦ä¹  æ— ç¤ºä¾‹ç›´æ¥æ¨ç† In-context learning ä¸Šä¸‹æ–‡å­¦ä¹  åŸºäºä¸Šä¸‹æ–‡çš„æ¨ç† Parameter-efficient fine-tuning å‚æ•°é«˜æ•ˆå¾®è°ƒ å‡å°‘å‚æ•°è°ƒæ•´çš„å¾®è°ƒæ–¹æ³• Model scaling æ¨¡å‹æ‰©å±• å¢åŠ æ¨¡å‹è§„æ¨¡ä»¥æå‡æ€§èƒ½ Compute-optimal scaling è®¡ç®—æœ€ä¼˜æ‰©å±• ä¼˜åŒ–è®¡ç®—èµ„æºçš„ä½¿ç”¨ Data-optimal scaling æ•°æ®æœ€ä¼˜æ‰©å±• ä¼˜åŒ–æ•°æ®ä½¿ç”¨çš„æ‰©å±• Encoder-decoder architecture ç¼–ç è§£ç å™¨æ¶æ„ æŸäº› LLM çš„æ¶æ„ Auto-regressive model è‡ªå›å½’æ¨¡å‹ ç”Ÿæˆæ–‡æœ¬çš„æ¨¡å‹ç±»å‹ Bidirectional model åŒå‘æ¨¡å‹ è€ƒè™‘å‰åæ–‡çš„æ¨¡å‹ Self-supervision è‡ªç›‘ç£ æ— éœ€æ ‡ç­¾çš„è®­ç»ƒæ–¹æ³• Unsupervised learning æ— ç›‘ç£å­¦ä¹  æ— æ ‡ç­¾æ•°æ®çš„å­¦ä¹  Supervised learning ç›‘ç£å­¦ä¹  æœ‰æ ‡ç­¾æ•°æ®çš„å­¦ä¹  "><title>LLMé¢†åŸŸçš„æœ¯è¯­åŠå…¶ä¸­æ–‡ç¿»è¯‘</title><link rel=canonical href=https://blog.yellster.top/p/llm-terminology-chinese-translations/><link rel=stylesheet href=/scss/style.min.946cca6c6259ef94ac55abfae7c7bf3291ea3ed5eea17ef77500b257217c6710.css><meta property='og:title' content="LLMé¢†åŸŸçš„æœ¯è¯­åŠå…¶ä¸­æ–‡ç¿»è¯‘"><meta property='og:description' content="æ¨¡å‹åç§°åŠå…¶ä¸­è‹±æ–‡å¯¹åº” è‹±æ–‡åç§° ä¸­æ–‡å¯¹åº” å…¨ç§° ä¸­æ–‡å…¨ç§°ç¿»è¯‘ GPT-3/GPT-4 GPT-3/GPT-4 Generative Pre-trained Transformer 3/4 ç”Ÿæˆå¼é¢„è®­ç»ƒå˜æ¢å™¨ 3/4 BERT BERT Bidirectional Encoder Representations from Transformers åŒå‘ç¼–ç å™¨è¡¨ç¤ºæ¥è‡ªå˜æ¢å™¨ RoBERTa RoBERTa Robustly Optimized BERT Pre-Training Approach ç¨³å¥ä¼˜åŒ– BERT é¢„è®­ç»ƒæ–¹æ³• XLNet XLNet XLNet XLNet T5 T5 Text-to-Text Transfer Transformer æ–‡æœ¬åˆ°æ–‡æœ¬è½¬ç§»å˜æ¢å™¨ BART BART Bidirectional and Auto-Regressive Transformers åŒå‘å’Œè‡ªå›å½’å˜æ¢å™¨ ELECTRA ELECTRA Efficiently Learning an Encoder that Classifies Token Replacements Accurately é«˜æ•ˆå­¦ä¹ ç¼–ç å™¨åˆ†ç±»ä»¤ç‰Œæ›¿æ¢å‡†ç¡® DeBERTa DeBERTa Decoding-enhanced BERT with Disentangled Attention è§£ç å¢å¼º BERT å…·æœ‰è§£è€¦æ³¨æ„åŠ› PaLM PaLM Pathways Language Model è·¯å¾„è¯­è¨€æ¨¡å‹ LLaMA LLaMA Large Language Model Meta AI å¤§å‹è¯­è¨€æ¨¡å‹ Meta AI BLOOM BLOOM BigScience Large Open-science Open-access Multilangual language model BigScience å¤§å‹å¼€æºå¤šè¯­è¨€è¯­è¨€æ¨¡å‹ Chinchilla Chinchilla Chinchilla Chinchilla OPT OPT Open Pre-trained Transformer å¼€æ”¾é¢„è®­ç»ƒå˜æ¢å™¨ ERNIE ERNIEï¼ˆæ–‡å¿ƒä¸€è¨€ï¼‰ Enhanced Representation through Knowledge Integration é€šè¿‡çŸ¥è¯†é›†æˆå¢å¼ºè¡¨ç¤º CPM CPM Chinese Pre-trained Model ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ GLM GLM General Language Model é€šç”¨è¯­è¨€æ¨¡å‹ MacBert MacBert MacBert MacBert Chinese BERT Chinese BERT Chinese BERT ä¸­æ–‡ BERT æœ¯è¯­åŠå…¶ä¸­è‹±æ–‡å¯¹åº” è‹±æ–‡æœ¯è¯­ ä¸­æ–‡ç¿»è¯‘ è¯´æ˜ AIGC äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ é€šè¿‡AIæŠ€æœ¯è‡ªåŠ¨ç”Ÿæˆçš„å„ç±»åª’ä½“å†…å®¹ï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰ï¼‰ Large Language Model å¤§å‹è¯­è¨€æ¨¡å‹ LLM çš„æ ¸å¿ƒæ¦‚å¿µ Pre training é¢„è®­ç»ƒ æ¨¡å‹åœ¨å¤§é‡æ•°æ®ä¸Šçš„åˆå§‹è®­ç»ƒ Fine-tuning å¾®è°ƒ åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè°ƒæ•´æ¨¡å‹ Transformer å˜æ¢å™¨ LLM å¸¸ç”¨çš„æ¶æ„ Attention mechanism æ³¨æ„åŠ›æœºåˆ¶ å˜æ¢å™¨ä¸­çš„å…³é”®æŠ€æœ¯ Word embedding è¯åµŒå…¥ å•è¯åˆ°å‘é‡è¡¨ç¤º Contextualized embedding ä¸Šä¸‹æ–‡åµŒå…¥ è€ƒè™‘ä¸Šä¸‹æ–‡çš„åµŒå…¥ Masked language modeling æ©ç è¯­è¨€å»ºæ¨¡ BERT ä½¿ç”¨çš„é¢„è®­ç»ƒä»»åŠ¡ Next sentence prediction ä¸‹ä¸€å¥é¢„æµ‹ BERT æ—©æœŸä½¿ç”¨çš„ä»»åŠ¡ Transfer learning è¿ç§»å­¦ä¹  æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡é—´çš„åº”ç”¨ Natural language processing è‡ªç„¶è¯­è¨€å¤„ç† LLM çš„åº”ç”¨é¢†åŸŸ Deep learning æ·±åº¦å­¦ä¹  LLM çš„åŸºç¡€æŠ€æœ¯ Neural network ç¥ç»ç½‘ç»œ æ·±åº¦å­¦ä¹ çš„æ„å»ºå— Gradient descent æ¢¯åº¦ä¸‹é™ ä¼˜åŒ–ç®—æ³• Backpropagation åå‘ä¼ æ’­ è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ–¹æ³• Prompt æç¤º è¾“å…¥ç»™æ¨¡å‹çš„æŒ‡ä»¤ Prompt engineering æç¤ºå·¥ç¨‹ ä¼˜åŒ–æç¤ºä»¥æ”¹å–„è¾“å‡º Few-shot learning å°‘æ ·æœ¬å­¦ä¹  å°‘é‡ç¤ºä¾‹ä¸‹å­¦ä¹  Zero-shot learning é›¶æ ·æœ¬å­¦ä¹  æ— ç¤ºä¾‹ç›´æ¥æ¨ç† In-context learning ä¸Šä¸‹æ–‡å­¦ä¹  åŸºäºä¸Šä¸‹æ–‡çš„æ¨ç† Parameter-efficient fine-tuning å‚æ•°é«˜æ•ˆå¾®è°ƒ å‡å°‘å‚æ•°è°ƒæ•´çš„å¾®è°ƒæ–¹æ³• Model scaling æ¨¡å‹æ‰©å±• å¢åŠ æ¨¡å‹è§„æ¨¡ä»¥æå‡æ€§èƒ½ Compute-optimal scaling è®¡ç®—æœ€ä¼˜æ‰©å±• ä¼˜åŒ–è®¡ç®—èµ„æºçš„ä½¿ç”¨ Data-optimal scaling æ•°æ®æœ€ä¼˜æ‰©å±• ä¼˜åŒ–æ•°æ®ä½¿ç”¨çš„æ‰©å±• Encoder-decoder architecture ç¼–ç è§£ç å™¨æ¶æ„ æŸäº› LLM çš„æ¶æ„ Auto-regressive model è‡ªå›å½’æ¨¡å‹ ç”Ÿæˆæ–‡æœ¬çš„æ¨¡å‹ç±»å‹ Bidirectional model åŒå‘æ¨¡å‹ è€ƒè™‘å‰åæ–‡çš„æ¨¡å‹ Self-supervision è‡ªç›‘ç£ æ— éœ€æ ‡ç­¾çš„è®­ç»ƒæ–¹æ³• Unsupervised learning æ— ç›‘ç£å­¦ä¹  æ— æ ‡ç­¾æ•°æ®çš„å­¦ä¹  Supervised learning ç›‘ç£å­¦ä¹  æœ‰æ ‡ç­¾æ•°æ®çš„å­¦ä¹  "><meta property='og:url' content='https://blog.yellster.top/p/llm-terminology-chinese-translations/'><meta property='og:site_name' content='Yellster - Blog'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='LLM'><meta property='article:tag' content='AIGC'><meta property='article:tag' content='AI'><meta property='article:tag' content='ä¸­æ–‡ç¿»è¯‘'><meta property='article:tag' content='æŠ€æœ¯æœ¯è¯­'><meta property='article:published_time' content='2025-03-24T22:30:59+08:00'><meta property='article:modified_time' content='2025-03-24T22:30:59+08:00'><meta name=twitter:title content="LLMé¢†åŸŸçš„æœ¯è¯­åŠå…¶ä¸­æ–‡ç¿»è¯‘"><meta name=twitter:description content="æ¨¡å‹åç§°åŠå…¶ä¸­è‹±æ–‡å¯¹åº” è‹±æ–‡åç§° ä¸­æ–‡å¯¹åº” å…¨ç§° ä¸­æ–‡å…¨ç§°ç¿»è¯‘ GPT-3/GPT-4 GPT-3/GPT-4 Generative Pre-trained Transformer 3/4 ç”Ÿæˆå¼é¢„è®­ç»ƒå˜æ¢å™¨ 3/4 BERT BERT Bidirectional Encoder Representations from Transformers åŒå‘ç¼–ç å™¨è¡¨ç¤ºæ¥è‡ªå˜æ¢å™¨ RoBERTa RoBERTa Robustly Optimized BERT Pre-Training Approach ç¨³å¥ä¼˜åŒ– BERT é¢„è®­ç»ƒæ–¹æ³• XLNet XLNet XLNet XLNet T5 T5 Text-to-Text Transfer Transformer æ–‡æœ¬åˆ°æ–‡æœ¬è½¬ç§»å˜æ¢å™¨ BART BART Bidirectional and Auto-Regressive Transformers åŒå‘å’Œè‡ªå›å½’å˜æ¢å™¨ ELECTRA ELECTRA Efficiently Learning an Encoder that Classifies Token Replacements Accurately é«˜æ•ˆå­¦ä¹ ç¼–ç å™¨åˆ†ç±»ä»¤ç‰Œæ›¿æ¢å‡†ç¡® DeBERTa DeBERTa Decoding-enhanced BERT with Disentangled Attention è§£ç å¢å¼º BERT å…·æœ‰è§£è€¦æ³¨æ„åŠ› PaLM PaLM Pathways Language Model è·¯å¾„è¯­è¨€æ¨¡å‹ LLaMA LLaMA Large Language Model Meta AI å¤§å‹è¯­è¨€æ¨¡å‹ Meta AI BLOOM BLOOM BigScience Large Open-science Open-access Multilangual language model BigScience å¤§å‹å¼€æºå¤šè¯­è¨€è¯­è¨€æ¨¡å‹ Chinchilla Chinchilla Chinchilla Chinchilla OPT OPT Open Pre-trained Transformer å¼€æ”¾é¢„è®­ç»ƒå˜æ¢å™¨ ERNIE ERNIEï¼ˆæ–‡å¿ƒä¸€è¨€ï¼‰ Enhanced Representation through Knowledge Integration é€šè¿‡çŸ¥è¯†é›†æˆå¢å¼ºè¡¨ç¤º CPM CPM Chinese Pre-trained Model ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ GLM GLM General Language Model é€šç”¨è¯­è¨€æ¨¡å‹ MacBert MacBert MacBert MacBert Chinese BERT Chinese BERT Chinese BERT ä¸­æ–‡ BERT æœ¯è¯­åŠå…¶ä¸­è‹±æ–‡å¯¹åº” è‹±æ–‡æœ¯è¯­ ä¸­æ–‡ç¿»è¯‘ è¯´æ˜ AIGC äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ é€šè¿‡AIæŠ€æœ¯è‡ªåŠ¨ç”Ÿæˆçš„å„ç±»åª’ä½“å†…å®¹ï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰ï¼‰ Large Language Model å¤§å‹è¯­è¨€æ¨¡å‹ LLM çš„æ ¸å¿ƒæ¦‚å¿µ Pre training é¢„è®­ç»ƒ æ¨¡å‹åœ¨å¤§é‡æ•°æ®ä¸Šçš„åˆå§‹è®­ç»ƒ Fine-tuning å¾®è°ƒ åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè°ƒæ•´æ¨¡å‹ Transformer å˜æ¢å™¨ LLM å¸¸ç”¨çš„æ¶æ„ Attention mechanism æ³¨æ„åŠ›æœºåˆ¶ å˜æ¢å™¨ä¸­çš„å…³é”®æŠ€æœ¯ Word embedding è¯åµŒå…¥ å•è¯åˆ°å‘é‡è¡¨ç¤º Contextualized embedding ä¸Šä¸‹æ–‡åµŒå…¥ è€ƒè™‘ä¸Šä¸‹æ–‡çš„åµŒå…¥ Masked language modeling æ©ç è¯­è¨€å»ºæ¨¡ BERT ä½¿ç”¨çš„é¢„è®­ç»ƒä»»åŠ¡ Next sentence prediction ä¸‹ä¸€å¥é¢„æµ‹ BERT æ—©æœŸä½¿ç”¨çš„ä»»åŠ¡ Transfer learning è¿ç§»å­¦ä¹  æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡é—´çš„åº”ç”¨ Natural language processing è‡ªç„¶è¯­è¨€å¤„ç† LLM çš„åº”ç”¨é¢†åŸŸ Deep learning æ·±åº¦å­¦ä¹  LLM çš„åŸºç¡€æŠ€æœ¯ Neural network ç¥ç»ç½‘ç»œ æ·±åº¦å­¦ä¹ çš„æ„å»ºå— Gradient descent æ¢¯åº¦ä¸‹é™ ä¼˜åŒ–ç®—æ³• Backpropagation åå‘ä¼ æ’­ è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ–¹æ³• Prompt æç¤º è¾“å…¥ç»™æ¨¡å‹çš„æŒ‡ä»¤ Prompt engineering æç¤ºå·¥ç¨‹ ä¼˜åŒ–æç¤ºä»¥æ”¹å–„è¾“å‡º Few-shot learning å°‘æ ·æœ¬å­¦ä¹  å°‘é‡ç¤ºä¾‹ä¸‹å­¦ä¹  Zero-shot learning é›¶æ ·æœ¬å­¦ä¹  æ— ç¤ºä¾‹ç›´æ¥æ¨ç† In-context learning ä¸Šä¸‹æ–‡å­¦ä¹  åŸºäºä¸Šä¸‹æ–‡çš„æ¨ç† Parameter-efficient fine-tuning å‚æ•°é«˜æ•ˆå¾®è°ƒ å‡å°‘å‚æ•°è°ƒæ•´çš„å¾®è°ƒæ–¹æ³• Model scaling æ¨¡å‹æ‰©å±• å¢åŠ æ¨¡å‹è§„æ¨¡ä»¥æå‡æ€§èƒ½ Compute-optimal scaling è®¡ç®—æœ€ä¼˜æ‰©å±• ä¼˜åŒ–è®¡ç®—èµ„æºçš„ä½¿ç”¨ Data-optimal scaling æ•°æ®æœ€ä¼˜æ‰©å±• ä¼˜åŒ–æ•°æ®ä½¿ç”¨çš„æ‰©å±• Encoder-decoder architecture ç¼–ç è§£ç å™¨æ¶æ„ æŸäº› LLM çš„æ¶æ„ Auto-regressive model è‡ªå›å½’æ¨¡å‹ ç”Ÿæˆæ–‡æœ¬çš„æ¨¡å‹ç±»å‹ Bidirectional model åŒå‘æ¨¡å‹ è€ƒè™‘å‰åæ–‡çš„æ¨¡å‹ Self-supervision è‡ªç›‘ç£ æ— éœ€æ ‡ç­¾çš„è®­ç»ƒæ–¹æ³• Unsupervised learning æ— ç›‘ç£å­¦ä¹  æ— æ ‡ç­¾æ•°æ®çš„å­¦ä¹  Supervised learning ç›‘ç£å­¦ä¹  æœ‰æ ‡ç­¾æ•°æ®çš„å­¦ä¹  "><link rel="shortcut icon" href=https://avatars.githubusercontent.com/u/37898221></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky compact"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=åˆ‡æ¢èœå•>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src="https://avatars.githubusercontent.com/u/37898221?v=4" width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ğŸ‡¨ğŸ‡³</span></figure><div class=site-meta><h1 class=site-name><a href=/>Yellster - Blog</a></h1><h2 class=site-description>æƒ³éƒ½æ˜¯é—®é¢˜ï¼Œåšæ‰æ˜¯ç­”æ¡ˆ</h2></div></header><ol class=menu-social><li><a href=https://github.com/Yellster target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>å…³äº</span></a></li><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>é¦–é¡µ</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>å½’æ¡£</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>æœç´¢</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>é“¾æ¥</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>æš—è‰²æ¨¡å¼</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">ç›®å½•</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#æ¨¡å‹åç§°åŠå…¶ä¸­è‹±æ–‡å¯¹åº”>æ¨¡å‹åç§°åŠå…¶ä¸­è‹±æ–‡å¯¹åº”</a></li><li><a href=#æœ¯è¯­åŠå…¶ä¸­è‹±æ–‡å¯¹åº”>æœ¯è¯­åŠå…¶ä¸­è‹±æ–‡å¯¹åº”</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/llm/>LLM
</a><a href=/categories/aigc/>AIGC
</a><a href=/categories/ai/>AI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/llm-terminology-chinese-translations/>LLMé¢†åŸŸçš„æœ¯è¯­åŠå…¶ä¸­æ–‡ç¿»è¯‘</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2025-03-24</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>é˜…è¯»æ—¶é•¿: 2 åˆ†é’Ÿ</time></div></footer></div></header><section class=article-content><h2 id=æ¨¡å‹åç§°åŠå…¶ä¸­è‹±æ–‡å¯¹åº”>æ¨¡å‹åç§°åŠå…¶ä¸­è‹±æ–‡å¯¹åº”</h2><div class=table-wrapper><table><thead><tr><th><strong>è‹±æ–‡åç§°</strong></th><th><strong>ä¸­æ–‡å¯¹åº”</strong></th><th><strong>å…¨ç§°</strong></th><th><strong>ä¸­æ–‡å…¨ç§°ç¿»è¯‘</strong></th></tr></thead><tbody><tr><td>GPT-3/GPT-4</td><td>GPT-3/GPT-4</td><td>Generative Pre-trained Transformer 3/4</td><td>ç”Ÿæˆå¼é¢„è®­ç»ƒå˜æ¢å™¨ 3/4</td></tr><tr><td>BERT</td><td>BERT</td><td>Bidirectional Encoder Representations from Transformers</td><td>åŒå‘ç¼–ç å™¨è¡¨ç¤ºæ¥è‡ªå˜æ¢å™¨</td></tr><tr><td>RoBERTa</td><td>RoBERTa</td><td>Robustly Optimized BERT Pre-Training Approach</td><td>ç¨³å¥ä¼˜åŒ– BERT é¢„è®­ç»ƒæ–¹æ³•</td></tr><tr><td>XLNet</td><td>XLNet</td><td>XLNet</td><td>XLNet</td></tr><tr><td>T5</td><td>T5</td><td>Text-to-Text Transfer Transformer</td><td>æ–‡æœ¬åˆ°æ–‡æœ¬è½¬ç§»å˜æ¢å™¨</td></tr><tr><td>BART</td><td>BART</td><td>Bidirectional and Auto-Regressive Transformers</td><td>åŒå‘å’Œè‡ªå›å½’å˜æ¢å™¨</td></tr><tr><td>ELECTRA</td><td>ELECTRA</td><td>Efficiently Learning an Encoder that Classifies Token Replacements Accurately</td><td>é«˜æ•ˆå­¦ä¹ ç¼–ç å™¨åˆ†ç±»ä»¤ç‰Œæ›¿æ¢å‡†ç¡®</td></tr><tr><td>DeBERTa</td><td>DeBERTa</td><td>Decoding-enhanced BERT with Disentangled Attention</td><td>è§£ç å¢å¼º BERT å…·æœ‰è§£è€¦æ³¨æ„åŠ›</td></tr><tr><td>PaLM</td><td>PaLM</td><td>Pathways Language Model</td><td>è·¯å¾„è¯­è¨€æ¨¡å‹</td></tr><tr><td>LLaMA</td><td>LLaMA</td><td>Large Language Model Meta AI</td><td>å¤§å‹è¯­è¨€æ¨¡å‹ Meta AI</td></tr><tr><td>BLOOM</td><td>BLOOM</td><td>BigScience Large Open-science Open-access Multilangual language model</td><td>BigScience å¤§å‹å¼€æºå¤šè¯­è¨€è¯­è¨€æ¨¡å‹</td></tr><tr><td>Chinchilla</td><td>Chinchilla</td><td>Chinchilla</td><td>Chinchilla</td></tr><tr><td>OPT</td><td>OPT</td><td>Open Pre-trained Transformer</td><td>å¼€æ”¾é¢„è®­ç»ƒå˜æ¢å™¨</td></tr><tr><td>ERNIE</td><td>ERNIEï¼ˆæ–‡å¿ƒä¸€è¨€ï¼‰</td><td>Enhanced Representation through Knowledge Integration</td><td>é€šè¿‡çŸ¥è¯†é›†æˆå¢å¼ºè¡¨ç¤º</td></tr><tr><td>CPM</td><td>CPM</td><td>Chinese Pre-trained Model</td><td>ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹</td></tr><tr><td>GLM</td><td>GLM</td><td>General Language Model</td><td>é€šç”¨è¯­è¨€æ¨¡å‹</td></tr><tr><td>MacBert</td><td>MacBert</td><td>MacBert</td><td>MacBert</td></tr><tr><td>Chinese BERT</td><td>Chinese BERT</td><td>Chinese BERT</td><td>ä¸­æ–‡ BERT</td></tr></tbody></table></div><h2 id=æœ¯è¯­åŠå…¶ä¸­è‹±æ–‡å¯¹åº”>æœ¯è¯­åŠå…¶ä¸­è‹±æ–‡å¯¹åº”</h2><div class=table-wrapper><table><thead><tr><th>è‹±æ–‡æœ¯è¯­</th><th>ä¸­æ–‡ç¿»è¯‘</th><th>è¯´æ˜</th></tr></thead><tbody><tr><td>AIGC</td><td>äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹</td><td>é€šè¿‡AIæŠ€æœ¯è‡ªåŠ¨ç”Ÿæˆçš„å„ç±»åª’ä½“å†…å®¹ï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰ï¼‰</td></tr><tr><td>Large Language Model</td><td>å¤§å‹è¯­è¨€æ¨¡å‹</td><td>LLM çš„æ ¸å¿ƒæ¦‚å¿µ</td></tr><tr><td>Pre training</td><td>é¢„è®­ç»ƒ</td><td>æ¨¡å‹åœ¨å¤§é‡æ•°æ®ä¸Šçš„åˆå§‹è®­ç»ƒ</td></tr><tr><td>Fine-tuning</td><td>å¾®è°ƒ</td><td>åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè°ƒæ•´æ¨¡å‹</td></tr><tr><td>Transformer</td><td>å˜æ¢å™¨</td><td>LLM å¸¸ç”¨çš„æ¶æ„</td></tr><tr><td>Attention mechanism</td><td>æ³¨æ„åŠ›æœºåˆ¶</td><td>å˜æ¢å™¨ä¸­çš„å…³é”®æŠ€æœ¯</td></tr><tr><td>Word embedding</td><td>è¯åµŒå…¥</td><td>å•è¯åˆ°å‘é‡è¡¨ç¤º</td></tr><tr><td>Contextualized embedding</td><td>ä¸Šä¸‹æ–‡åµŒå…¥</td><td>è€ƒè™‘ä¸Šä¸‹æ–‡çš„åµŒå…¥</td></tr><tr><td>Masked language modeling</td><td>æ©ç è¯­è¨€å»ºæ¨¡</td><td>BERT ä½¿ç”¨çš„é¢„è®­ç»ƒä»»åŠ¡</td></tr><tr><td>Next sentence prediction</td><td>ä¸‹ä¸€å¥é¢„æµ‹</td><td>BERT æ—©æœŸä½¿ç”¨çš„ä»»åŠ¡</td></tr><tr><td>Transfer learning</td><td>è¿ç§»å­¦ä¹ </td><td>æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡é—´çš„åº”ç”¨</td></tr><tr><td>Natural language processing</td><td>è‡ªç„¶è¯­è¨€å¤„ç†</td><td>LLM çš„åº”ç”¨é¢†åŸŸ</td></tr><tr><td>Deep learning</td><td>æ·±åº¦å­¦ä¹ </td><td>LLM çš„åŸºç¡€æŠ€æœ¯</td></tr><tr><td>Neural network</td><td>ç¥ç»ç½‘ç»œ</td><td>æ·±åº¦å­¦ä¹ çš„æ„å»ºå—</td></tr><tr><td>Gradient descent</td><td>æ¢¯åº¦ä¸‹é™</td><td>ä¼˜åŒ–ç®—æ³•</td></tr><tr><td>Backpropagation</td><td>åå‘ä¼ æ’­</td><td>è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ–¹æ³•</td></tr><tr><td>Prompt</td><td>æç¤º</td><td>è¾“å…¥ç»™æ¨¡å‹çš„æŒ‡ä»¤</td></tr><tr><td>Prompt engineering</td><td>æç¤ºå·¥ç¨‹</td><td>ä¼˜åŒ–æç¤ºä»¥æ”¹å–„è¾“å‡º</td></tr><tr><td>Few-shot learning</td><td>å°‘æ ·æœ¬å­¦ä¹ </td><td>å°‘é‡ç¤ºä¾‹ä¸‹å­¦ä¹ </td></tr><tr><td>Zero-shot learning</td><td>é›¶æ ·æœ¬å­¦ä¹ </td><td>æ— ç¤ºä¾‹ç›´æ¥æ¨ç†</td></tr><tr><td>In-context learning</td><td>ä¸Šä¸‹æ–‡å­¦ä¹ </td><td>åŸºäºä¸Šä¸‹æ–‡çš„æ¨ç†</td></tr><tr><td>Parameter-efficient fine-tuning</td><td>å‚æ•°é«˜æ•ˆå¾®è°ƒ</td><td>å‡å°‘å‚æ•°è°ƒæ•´çš„å¾®è°ƒæ–¹æ³•</td></tr><tr><td>Model scaling</td><td>æ¨¡å‹æ‰©å±•</td><td>å¢åŠ æ¨¡å‹è§„æ¨¡ä»¥æå‡æ€§èƒ½</td></tr><tr><td>Compute-optimal scaling</td><td>è®¡ç®—æœ€ä¼˜æ‰©å±•</td><td>ä¼˜åŒ–è®¡ç®—èµ„æºçš„ä½¿ç”¨</td></tr><tr><td>Data-optimal scaling</td><td>æ•°æ®æœ€ä¼˜æ‰©å±•</td><td>ä¼˜åŒ–æ•°æ®ä½¿ç”¨çš„æ‰©å±•</td></tr><tr><td>Encoder-decoder architecture</td><td>ç¼–ç è§£ç å™¨æ¶æ„</td><td>æŸäº› LLM çš„æ¶æ„</td></tr><tr><td>Auto-regressive model</td><td>è‡ªå›å½’æ¨¡å‹</td><td>ç”Ÿæˆæ–‡æœ¬çš„æ¨¡å‹ç±»å‹</td></tr><tr><td>Bidirectional model</td><td>åŒå‘æ¨¡å‹</td><td>è€ƒè™‘å‰åæ–‡çš„æ¨¡å‹</td></tr><tr><td>Self-supervision</td><td>è‡ªç›‘ç£</td><td>æ— éœ€æ ‡ç­¾çš„è®­ç»ƒæ–¹æ³•</td></tr><tr><td>Unsupervised learning</td><td>æ— ç›‘ç£å­¦ä¹ </td><td>æ— æ ‡ç­¾æ•°æ®çš„å­¦ä¹ </td></tr><tr><td>Supervised learning</td><td>ç›‘ç£å­¦ä¹ </td><td>æœ‰æ ‡ç­¾æ•°æ®çš„å­¦ä¹ </td></tr></tbody></table></div></section><footer class=article-footer><section class=article-tags><a href=/tags/llm/>LLM</a>
<a href=/tags/aigc/>AIGC</a>
<a href=/tags/ai/>AI</a>
<a href=/tags/%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91/>ä¸­æ–‡ç¿»è¯‘</a>
<a href=/tags/%E6%8A%80%E6%9C%AF%E6%9C%AF%E8%AF%AD/>æŠ€æœ¯æœ¯è¯­</a></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><footer class=site-footer><section class=copyright>&copy;
2023 -
2025 Yellster - Blog</section><section class=powerby>æƒ³éƒ½æ˜¯é—®é¢˜ï¼Œåšæ‰æ˜¯ç­”æ¡ˆï¼<br>ä½¿ç”¨ <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> æ„å»º<br>ä¸»é¢˜ <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> ç”± <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> è®¾è®¡</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>