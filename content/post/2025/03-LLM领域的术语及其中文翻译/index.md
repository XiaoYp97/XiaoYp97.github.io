---
title: "LLM领域的术语及其中文翻译"
description:
date: "2025-03-24T22:30:59+08:00"
slug: "llm-terminology-chinese-translations"
image: ""
license: false
hidden: false
comments: false
draft: false
tags: ["LLM", "AIGC", "AI", "中文翻译", "技术术语"]
categories: ["LLM", "AIGC", "AI"]
# weight: 1 # You can add weight to some posts to override the default sorting (date descending)
---

## 模型名称及其中英文对应

| **英文名称**    | **中文对应**      | **全称**                                      | **中文全称翻译**                          |
|---------------|------------------|----------------------------------------------|------------------------------------------|
| GPT-3/GPT-4   | GPT-3/GPT-4      | Generative Pre-trained Transformer 3/4        | 生成式预训练变换器 3/4                    |
| BERT          | BERT             | Bidirectional Encoder Representations from Transformers | 双向编码器表示来自变换器                  |
| RoBERTa       | RoBERTa          | Robustly Optimized BERT Pre-Training Approach | 稳健优化 BERT 预训练方法                  |
| XLNet         | XLNet            | XLNet                                        | XLNet                                    |
| T5            | T5               | Text-to-Text Transfer Transformer            | 文本到文本转移变换器                      |
| BART          | BART             | Bidirectional and Auto-Regressive Transformers | 双向和自回归变换器                        |
| ELECTRA       | ELECTRA          | Efficiently Learning an Encoder that Classifies Token Replacements Accurately | 高效学习编码器分类令牌替换准确|
| DeBERTa       | DeBERTa          | Decoding-enhanced BERT with Disentangled Attention | 解码增强 BERT 具有解耦注意力              |
| PaLM          | PaLM             | Pathways Language Model                      | 路径语言模型                              |
| LLaMA         | LLaMA            | Large Language Model Meta AI                 | 大型语言模型 Meta AI                      |
| BLOOM         | BLOOM            | BigScience Large Open-science Open-access Multilangual language model | BigScience 大型开源多语言语言模型|
| Chinchilla    | Chinchilla       | Chinchilla                                   | Chinchilla                                |
| OPT           | OPT              | Open Pre-trained Transformer                 | 开放预训练变换器                          |
| ERNIE         | ERNIE（文心一言）  | Enhanced Representation through Knowledge Integration | 通过知识集成增强表示                      |
| CPM           | CPM              | Chinese Pre-trained Model                    | 中文预训练模型                            |
| GLM           | GLM              | General Language Model                       | 通用语言模型                              |
| MacBert       | MacBert          | MacBert                                      | MacBert                                  |
| Chinese BERT  | Chinese BERT     | Chinese BERT                                 | 中文 BERT                                 |

## 术语及其中英文对应

| 英文术语                | 中文翻译          | 说明                          |
|-------------------------|-------------------|-------------------------------|
| AIGC                    | 人工智能生成内容 | 通过AI技术自动生成的各类媒体内容（文本、图像、音频等） |
| Large Language Model    | 大型语言模型      | LLM 的核心概念                |
| Pre training            | 预训练           | 模型在大量数据上的初始训练     |
| Fine-tuning             | 微调             | 在特定任务上调整模型           |
| Transformer             | 变换器           | LLM 常用的架构                |
| Attention mechanism     | 注意力机制       | 变换器中的关键技术             |
| Word embedding          | 词嵌入           | 单词到向量表示                |
| Contextualized embedding| 上下文嵌入       | 考虑上下文的嵌入              |
| Masked language modeling| 掩码语言建模     | BERT 使用的预训练任务         |
| Next sentence prediction| 下一句预测       | BERT 早期使用的任务           |
| Transfer learning       | 迁移学习         | 模型在不同任务间的应用         |
| Natural language processing| 自然语言处理| LLM 的应用领域                |
| Deep learning           | 深度学习         | LLM 的基础技术                |
| Neural network          | 神经网络         | 深度学习的构建块              |
| Gradient descent        | 梯度下降         | 优化算法                      |
| Backpropagation         | 反向传播         | 训练神经网络的核心方法        |
| Prompt                  | 提示             | 输入给模型的指令              |
| Prompt engineering      | 提示工程         | 优化提示以改善输出            |
| Few-shot learning       | 少样本学习       | 少量示例下学习                |
| Zero-shot learning      | 零样本学习       | 无示例直接推理                |
| In-context learning     | 上下文学习       | 基于上下文的推理              |
| Parameter-efficient fine-tuning| 参数高效微调| 减少参数调整的微调方法        |
| Model scaling           | 模型扩展         | 增加模型规模以提升性能        |
| Compute-optimal scaling | 计算最优扩展     | 优化计算资源的使用            |
| Data-optimal scaling    | 数据最优扩展     | 优化数据使用的扩展            |
| Encoder-decoder architecture| 编码解码器架构| 某些 LLM 的架构               |
| Auto-regressive model   | 自回归模型       | 生成文本的模型类型            |
| Bidirectional model     | 双向模型         | 考虑前后文的模型              |
| Self-supervision        | 自监督           | 无需标签的训练方法            |
| Unsupervised learning   | 无监督学习       | 无标签数据的学习              |
| Supervised learning     | 监督学习         | 有标签数据的学习              |
